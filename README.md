<div align="center">
<h2><font color="red"> Follow-Your-Motion </font></center> <br> <center>Video Motion Transfer via Efficient Spatial-Temporal Decoupled Finetuning</h2>

[Yue Ma*](https://mayuelala.github.io/), [Yulong Liu*](https://scholar.google.com.my/citations?user=GZC_7c4AAAAJ), [Qiyuan Zhu*](https://github.com/follow-your-motion), [Ayden Yang](https://github.com/follow-your-motion), [Kunyu Feng](https://github.com/fkyyyy), [Xinhua Zhang](https://github.com/NXZXH), [Zhifeng Li](https://scholar.google.com/citations?user=VTrRNN4AAAAJ&hl=zh-CN),  
[Sirui Han](https://facultyprofiles.hkust.edu.hk/profiles.php?profile=sirui-han-siruihan), [Chenyang Qi](https://chenyangqiqi.github.io/) and [Qifeng Chen](https://scholar.google.com/citations?user=lLMX9hcAAAAJ&hl=en)

<a href='https://arxiv.org/abs/2506.05207'><img src='https://img.shields.io/badge/ArXiv-2403.08268-red'></a> 
<a href='https://follow-your-motion.github.io/'><img src='https://img.shields.io/badge/Project-Page-Green'></a>  
![visitors](https://visitor-badge.laobi.icu/badge?page_id=mayuelala.FollowYourMotion&left_color=green&right_color=red)  [![GitHub](https://img.shields.io/github/stars/mayuelala/FollowYourMotion?style=social)](https://github.com/mayuelala/FollowYourMotion) 
</div>


# 🖼 Gallery

We have showcased some results of Follow-Your-Motion .

More results can be found on our [Project page](https://follow-your-motion.github.io/).


<table>
  <tr>
    <td><video src="./assets/1.mp4" type="video/mp4" autoplay muted loop></video></td>
  </tr>
  <tr>
    <td><video src="./assets/3.mp4" type="video/mp4" autoplay muted loop></video></td>
  </tr>
    <tr>
    <td><video src="./assets/5.mp4" type="video/mp4" autoplay muted loop></video></td>
  </tr>
    <tr>
    <td><video src="./assets/7.mp4" type="video/mp4" autoplay muted loop></video></td>
  </tr>
    <tr>
    <td><video src="./assets/9.mp4" type="video/mp4" autoplay muted loop></video></td>
  </tr>
    <tr>
    <td><video src="./assets/11.mp4" type="video/mp4" autoplay muted loop></video></td>
  </tr>
    <tr>
    <td><video src="./assets/13.mp4" type="video/mp4" autoplay muted loop></video></td>
  </tr>
    <tr>
    <td><video src="./assets/15.mp4" type="video/mp4" autoplay muted loop></video></td>
  </tr>
    <tr>
    <td><video src="./assets/17.mp4" type="video/mp4" autoplay muted loop></video></td>
  </tr>
    <tr>
    <td><video src="./assets/19.mp4" type="video/mp4" autoplay muted loop></video></td>
  </tr>
    <tr>
    <td><video src="./assets/21.mp4" type="video/mp4" autoplay muted loop></video></td>
  </tr>
    <tr>
    <td><video src="./assets/23.mp4" type="video/mp4" autoplay muted loop></video></td>
  </tr>
    <tr>
    <td><video src="./assets/25.mp4" type="video/mp4" autoplay muted loop></video></td>
  </tr>
  <tr>
    <td><video src="./assets/27.mp4" type="video/mp4" autoplay muted loop></video></td>
  </tr>
  <tr>
    <td><video src="./assets/29.mp4" type="video/mp4" autoplay muted loop></video></td>
  </tr>

</table>


# 📍 Note  
### 🕹 We are cleaning the code and creating a demo. We really want everybody to try it! 
### 😊 The code and checkpoints is coming soon！
### 💗 Thanks for your attention! If you are interested in our work, please give us a star ⭐️⭐️⭐ to let us know.
### 🚀 We will speed up the development! 


# 👨‍👩‍👧‍👦 Follow Family
[Follow-Your-Pose](https://github.com/mayuelala/FollowYourPose): Pose-Guided text-to-Video Generation.

[Follow-Your-Click](https://github.com/mayuelala/FollowYourClick): Open-domain Regional image animation via Short Prompts.
  

## ⭐️ Star History

[![Star History Chart](https://api.star-history.com/svg?repos=mayuelala/FollowYourMotion&type=Date)](https://star-history.com/#mayuelala/FollowYourMotion&Date)



# 🎼 Citation 
If you think this project is helpful, please feel free to leave a star⭐️⭐️⭐️ and cite our paper:
```bibtex
@article{ma2025follow,
  title={Follow-Your-Motion: Video Motion Transfer via Efficient Spatial-Temporal Decoupled Finetuning},
  author={Ma, Yue and Liu, Yulong and Zhu, Qiyuan and Yang, Ayden and Feng, Kunyu and Zhang, Xinhua and Li, Zhifeng and Han, Sirui and Qi, Chenyang and Chen, Qifeng},
  journal={arXiv preprint arXiv:2506.05207},
  year={2025}
}
``` 

